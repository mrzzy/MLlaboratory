{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grocery Problem: Preparing Data\n",
    "In this notebook, we preprocess the dataset for machine learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports & setup envrionment "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2 \n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import time\n",
    "import pickle\n",
    "import numpy as np\n",
    "import matplotlib as plt\n",
    "import pandas as pd\n",
    "\n",
    "from datetime import date\n",
    "\n",
    "from fastai.imports import *\n",
    "from fastai.structured import *\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\n",
    "from IPython.display import display # extract a feature record from each date\n",
    "from sklearn import metrics\n",
    "from sklearn import preprocessing\n",
    "\n",
    "from multiprocessing import cpu_count, Pool\n",
    "from multiprocessing.pool import ThreadPool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEBUG_MODE = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utilities\n",
    "Some utilities to make it easier/faster to work with pandas \n",
    "##### parallelise pandas operations to better load the cpu\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parallel apply the given function to the given dataframe.\n",
    "# Splits the dataframe into  the given number of splits \n",
    "# and applys the function to each split concurrently on n_procs processes\n",
    "def parallel_apply(func, df, n_splits=cpu_count() * 8, n_procs=cpu_count()):\n",
    "    # Split dataframe into n_split splits\n",
    "    df_splits = np.array_split(df, n_splits)\n",
    "    # Apply the function using process pool\n",
    "    pool = Pool(processes=n_procs)\n",
    "    df_splits = pool.map(func, df_splits)\n",
    "    pool.close()\n",
    "    # Merge results into single dataframe\n",
    "    df = pd.concat(df_splits)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess data\n",
    "\n",
    "### Load Data\n",
    "Load the data processed from the previous (grocery_problem_data_preprocessing.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 125497040 training examples, 3370464 test examples\n"
     ]
    }
   ],
   "source": [
    "PATH = \"tmp/groceries/\"\n",
    "df_train = pd.read_feather(os.path.join(PATH, \"train.feather\"))\n",
    "df_test = pd.read_feather(os.path.join(PATH, \"test.feather\"))\n",
    "\n",
    "print(f\"Loaded {len(df_train)} training examples, {len(df_test)} test examples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>unique</th>\n",
       "      <th>top</th>\n",
       "      <th>freq</th>\n",
       "      <th>first</th>\n",
       "      <th>last</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>min</th>\n",
       "      <th>25%</th>\n",
       "      <th>50%</th>\n",
       "      <th>75%</th>\n",
       "      <th>max</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <td>100000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6.27324e+07</td>\n",
       "      <td>3.61029e+07</td>\n",
       "      <td>922</td>\n",
       "      <td>3.15609e+07</td>\n",
       "      <td>6.25638e+07</td>\n",
       "      <td>9.40351e+07</td>\n",
       "      <td>1.25495e+08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>date</th>\n",
       "      <td>100000</td>\n",
       "      <td>1682</td>\n",
       "      <td>2016-12-23 00:00:00</td>\n",
       "      <td>109</td>\n",
       "      <td>2013-01-02 00:00:00</td>\n",
       "      <td>2017-08-15 00:00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>store_nbr</th>\n",
       "      <td>100000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>27.5094</td>\n",
       "      <td>16.3756</td>\n",
       "      <td>1</td>\n",
       "      <td>12</td>\n",
       "      <td>28</td>\n",
       "      <td>43</td>\n",
       "      <td>54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>item_nbr</th>\n",
       "      <td>100000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>973702</td>\n",
       "      <td>519555</td>\n",
       "      <td>96995</td>\n",
       "      <td>551893</td>\n",
       "      <td>959500</td>\n",
       "      <td>1.35397e+06</td>\n",
       "      <td>2.12219e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unit_sales</th>\n",
       "      <td>100000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8.72572</td>\n",
       "      <td>29.3038</td>\n",
       "      <td>-1</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>9</td>\n",
       "      <td>7001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>onpromotion</th>\n",
       "      <td>100000</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>100000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>family</th>\n",
       "      <td>100000</td>\n",
       "      <td>32</td>\n",
       "      <td>GROCERY I</td>\n",
       "      <td>36656</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>class</th>\n",
       "      <td>100000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1955.76</td>\n",
       "      <td>1140.29</td>\n",
       "      <td>1002</td>\n",
       "      <td>1048</td>\n",
       "      <td>1190</td>\n",
       "      <td>2712</td>\n",
       "      <td>7780</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>perishable</th>\n",
       "      <td>100000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.25229</td>\n",
       "      <td>0.434329</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>city</th>\n",
       "      <td>100000</td>\n",
       "      <td>22</td>\n",
       "      <td>Quito</td>\n",
       "      <td>41746</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>state</th>\n",
       "      <td>100000</td>\n",
       "      <td>16</td>\n",
       "      <td>Pichincha</td>\n",
       "      <td>43728</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>type_x</th>\n",
       "      <td>100000</td>\n",
       "      <td>5</td>\n",
       "      <td>D</td>\n",
       "      <td>37417</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cluster</th>\n",
       "      <td>100000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8.73391</td>\n",
       "      <td>4.63604</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>9</td>\n",
       "      <td>13</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dcoilwtico</th>\n",
       "      <td>100000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>61.2425</td>\n",
       "      <td>23.7055</td>\n",
       "      <td>26.19</td>\n",
       "      <td>45.4133</td>\n",
       "      <td>49.95</td>\n",
       "      <td>90.74</td>\n",
       "      <td>110.62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>type_y</th>\n",
       "      <td>100000</td>\n",
       "      <td>6</td>\n",
       "      <td>Holiday</td>\n",
       "      <td>72595</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>locale</th>\n",
       "      <td>100000</td>\n",
       "      <td>3</td>\n",
       "      <td>National</td>\n",
       "      <td>49151</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>transferred</th>\n",
       "      <td>100000</td>\n",
       "      <td>2</td>\n",
       "      <td>False</td>\n",
       "      <td>98348</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              count unique                  top    freq                first  \\\n",
       "id           100000    NaN                  NaN     NaN                  NaN   \n",
       "date         100000   1682  2016-12-23 00:00:00     109  2013-01-02 00:00:00   \n",
       "store_nbr    100000    NaN                  NaN     NaN                  NaN   \n",
       "item_nbr     100000    NaN                  NaN     NaN                  NaN   \n",
       "unit_sales   100000    NaN                  NaN     NaN                  NaN   \n",
       "onpromotion  100000      1                 True  100000                  NaN   \n",
       "family       100000     32            GROCERY I   36656                  NaN   \n",
       "class        100000    NaN                  NaN     NaN                  NaN   \n",
       "perishable   100000    NaN                  NaN     NaN                  NaN   \n",
       "city         100000     22                Quito   41746                  NaN   \n",
       "state        100000     16            Pichincha   43728                  NaN   \n",
       "type_x       100000      5                    D   37417                  NaN   \n",
       "cluster      100000    NaN                  NaN     NaN                  NaN   \n",
       "dcoilwtico   100000    NaN                  NaN     NaN                  NaN   \n",
       "type_y       100000      6              Holiday   72595                  NaN   \n",
       "locale       100000      3             National   49151                  NaN   \n",
       "transferred  100000      2                False   98348                  NaN   \n",
       "\n",
       "                            last         mean          std    min  \\\n",
       "id                           NaN  6.27324e+07  3.61029e+07    922   \n",
       "date         2017-08-15 00:00:00          NaN          NaN    NaN   \n",
       "store_nbr                    NaN      27.5094      16.3756      1   \n",
       "item_nbr                     NaN       973702       519555  96995   \n",
       "unit_sales                   NaN      8.72572      29.3038     -1   \n",
       "onpromotion                  NaN          NaN          NaN    NaN   \n",
       "family                       NaN          NaN          NaN    NaN   \n",
       "class                        NaN      1955.76      1140.29   1002   \n",
       "perishable                   NaN      0.25229     0.434329      0   \n",
       "city                         NaN          NaN          NaN    NaN   \n",
       "state                        NaN          NaN          NaN    NaN   \n",
       "type_x                       NaN          NaN          NaN    NaN   \n",
       "cluster                      NaN      8.73391      4.63604      1   \n",
       "dcoilwtico                   NaN      61.2425      23.7055  26.19   \n",
       "type_y                       NaN          NaN          NaN    NaN   \n",
       "locale                       NaN          NaN          NaN    NaN   \n",
       "transferred                  NaN          NaN          NaN    NaN   \n",
       "\n",
       "                     25%          50%          75%          max  \n",
       "id           3.15609e+07  6.25638e+07  9.40351e+07  1.25495e+08  \n",
       "date                 NaN          NaN          NaN          NaN  \n",
       "store_nbr             12           28           43           54  \n",
       "item_nbr          551893       959500  1.35397e+06  2.12219e+06  \n",
       "unit_sales             2            4            9         7001  \n",
       "onpromotion          NaN          NaN          NaN          NaN  \n",
       "family               NaN          NaN          NaN          NaN  \n",
       "class               1048         1190         2712         7780  \n",
       "perishable             0            0            1            1  \n",
       "city                 NaN          NaN          NaN          NaN  \n",
       "state                NaN          NaN          NaN          NaN  \n",
       "type_x               NaN          NaN          NaN          NaN  \n",
       "cluster                4            9           13           17  \n",
       "dcoilwtico       45.4133        49.95        90.74       110.62  \n",
       "type_y               NaN          NaN          NaN          NaN  \n",
       "locale               NaN          NaN          NaN          NaN  \n",
       "transferred          NaN          NaN          NaN          NaN  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.sample(100000).describe(include=\"all\").T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Drop Nans in the dataset\n",
    "We have introduced Nans in the dataset when merging the different dataframes.\n",
    "\n",
    "To solve this we drop the Nan's we have added in the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 13.6 s, sys: 30 s, total: 43.7 s\n",
      "Wall time: 41.8 s\n",
      "CPU times: user 789 ms, sys: 393 ms, total: 1.18 s\n",
      "Wall time: 1.92 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "187"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def rm_nan(df): return df.dropna()\n",
    "\n",
    "%time df_train = parallel_apply(rm_nan, df_train)\n",
    "%time df_test = rm_nan(df_test)\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Selection\n",
    "More operations on such a large dataset are going to be incredibly expensive. Things we can do:\n",
    "1. Parallelise and scale to process all the data (dask, spark, hadoop)\n",
    "2. Sample dataset to get most important data, resulting in a computational  managable sample \n",
    "\n",
    "Picking option 2, we sample the datset.\n",
    "one option is to randomly sample the dataset.\n",
    "However, some heursitics can give us a better sample.\n",
    "The task of the competition is to predict the next two weeks after the end of the training set. Intutively, we sort by date and sample the latests data:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample latests data\n",
    "SAMPLE_SIZE = 10 * int(1e+6) # sample 10 million\n",
    "# due to a previous operation, we have already sorted the data\n",
    "#df_train.sort_values(by=\"date\", inplace=True) \n",
    "df_train = df_train.tail(SAMPLE_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Extraction\n",
    "Taking a look at the dataset, there are still some feature extraction we need before we can fit a model\n",
    "\n",
    "1. Extract numeric features from the the `date` column\n",
    "2. Convert categorical features into numeric features\n",
    "3. Convert boolean features into 0 and 1s."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature extraction of the date column\n",
    "One way to apporach this is convert date to the number of days from the epoch (ie the first date in the dataset)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract date day offsets feature for the date series\n",
    "# Returns the extracted day offests series\n",
    "def extract_date_feature(dates):\n",
    "    min_date = dates.min()\n",
    "    day_offsets = dates.apply((lambda d:(d - min_date).days))\n",
    "    return day_offsets\n",
    "\n",
    "# Add a date day offsets feature to the  given dataframe\n",
    "# derieved from the dates on the given column name col_name\n",
    "# Returns the dataframe with the date day offsets feature as date\n",
    "def convert_date_feature(df, col_name):\n",
    "    dates =  df[col_name]\n",
    "    day_offsets = parallel_apply(extract_date_feature, dates)\n",
    "    #day_offsets = extract_date_feature(dates)\n",
    "    new_df = df.copy()\n",
    "    new_df[col_name] = day_offsets\n",
    "    return new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.02 s ± 181 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "if DEBUG_MODE:\n",
    "    df = df_train.tail(int(1e+6))\n",
    "    %timeit convert_date_feature(df, \"date\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 436 ms, sys: 2.68 s, total: 3.11 s\n",
      "Wall time: 30.7 s\n",
      "CPU times: user 219 ms, sys: 2.29 s, total: 2.51 s\n",
      "Wall time: 12.4 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "21"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%time df_train = convert_date_feature(df_train, \"date\")\n",
    "%time df_test = convert_date_feature(df_test, \"date\")\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convert categorical features to numerical\n",
    "Convert categorical classes to numerical features\n",
    "\n",
    "In this cause, we will be using the one hot encoding method to encoded the  features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the categorical columns in the dataframe in place \n",
    "# to one encoded categories\n",
    "def convert_one_hot(df):\n",
    "    # Find names of categorical columns\n",
    "    cat_columns = [ column for column in df.columns\n",
    "                   if df[column].dtype.__class__ is pd.CategoricalDtype]\n",
    "\n",
    "    # Encode categorical variables into categorical variables\n",
    "    encoded_df = df.copy()\n",
    "    for column in cat_columns:\n",
    "        encoding = pd.get_dummies(df[column], prefix=column)\n",
    "        # remove original column and replace with one hot encoding\n",
    "        encoded_df = encoded_df.drop(columns=[column])\n",
    "        encoded_df = pd.concat([encoded_df, encoding], axis=1)\n",
    "\n",
    "    \n",
    "    return encoded_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.04 s ± 31.9 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "if DEBUG_MODE:\n",
    "    df = df_train.sample(int(1e+6))\n",
    "    %timeit convert_one_hot(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 5.99 s, sys: 6.05 s, total: 12 s\n",
      "Wall time: 12 s\n",
      "CPU times: user 2.15 s, sys: 1.61 s, total: 3.76 s\n",
      "Wall time: 3.76 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "202"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%time df_train = convert_one_hot(df_train)\n",
    "%time df_test = convert_one_hot(df_test)\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convert Boolean features to numeric\n",
    "Finally, we convert the Boolean features from (True, False) to (1, 0) respectively"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# converts the Boolean features from (True, False) to (1, 0) respectively\n",
    "def convert_boolean(df):\n",
    "    # Find boolean columns\n",
    "    bool_columns = [ column for column in df.columns\n",
    "                    if df[column].dtype.name == \"bool\" ]\n",
    "\n",
    "    # Convert boolean columns to numeric integers\n",
    "    for column in bool_columns:\n",
    "        df[column] = df[column].astype(int)\n",
    "        \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "806 µs ± 7.91 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)\n"
     ]
    }
   ],
   "source": [
    "if DEBUG_MODE:\n",
    "    df = df_train.sample(int(1e+6))\n",
    "    %timeit convert_boolean(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 74.1 ms, sys: 0 ns, total: 74.1 ms\n",
      "Wall time: 71.3 ms\n",
      "CPU times: user 23 ms, sys: 0 ns, total: 23 ms\n",
      "Wall time: 22.8 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "49"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%time df_train = convert_boolean(df_train)\n",
    "%time df_test = convert_boolean(df_test)\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Scaling\n",
    "Some Machine Learning algorihms perform poorly on unscaled data.So we scale the features in the data by 1. Zeroing the mean and 2. Subtracting the standard deviation\n",
    "\n",
    "We will do this with sckit-learn's `StandardScaler`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/sklearn/preprocessing/data.py:645: DataConversionWarning: Data with input dtype uint8, int8, int16, float32, int32, int64 were all converted to float64 by StandardScaler.\n",
      "  return self.partial_fit(X, y)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 11.9 s, sys: 1min 12s, total: 1min 24s\n",
      "Wall time: 1min 49s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "StandardScaler(copy=False, with_mean=True, with_std=True)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create and fit to training data\n",
    "scaler = StandardScaler(with_mean=True, with_std=True, copy=False)\n",
    "%time scaler.fit(df_train.drop(columns=\"unit_sales\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale both the training and test data\n",
    "def scale_features(df):\n",
    "    unit_sales = None\n",
    "    if \"unit_sales\" in df.columns:\n",
    "        unit_sales = df[\"unit_sales\"]\n",
    "        df = df.drop(columns=\"unit_sales\")\n",
    "     \n",
    "    scaled_feats = scaler.transform(df)\n",
    "    df = pd.DataFrame(scaled_feats, index=df.index, columns=df.columns)\n",
    "    \n",
    "    if not unit_sales is None:\n",
    "        df = pd.concat([df, unit_sales], axis=1)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:8: DataConversionWarning: Data with input dtype uint8, int8, int16, float32, int32, int64 were all converted to float64 by StandardScaler.\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4.66 s, sys: 15 s, total: 19.6 s\n",
      "Wall time: 26.5 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:8: DataConversionWarning: Data with input dtype uint8, int8, int16, float32, int32, int64 were all converted to float64 by StandardScaler.\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.1 s, sys: 2.38 s, total: 3.48 s\n",
      "Wall time: 6.71 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "74"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%time df_train = scale_features(df_train)\n",
    "%time df_test = scale_features(df_test)\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finishing Up and Saving data\n",
    "We are finished with preprocessing the data.\n",
    "\n",
    "Commit the data to disk for training models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset index to because if not feather will complain\n",
    "df_train = df_train.reset_index()\n",
    "df_test = df_test.reset_index()    \n",
    "\n",
    "# commit to disk in feather format\n",
    "df_train.to_feather(os.path.join(PATH, \"train_pp.feather\"))\n",
    "df_test.to_feather(os.path.join(PATH, \"test_pp.feather\"))\n",
    "\n",
    "# save fitted scaler for later use\n",
    "with open(os.path.join(PATH, \"scaler.pickle\"), \"wb\") as f:\n",
    "    pickle.dump(scaler, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
